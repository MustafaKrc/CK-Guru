services:
  #################
  # Backend API
  #################
  backend:
    build:
      context: .
      dockerfile: backend/Dockerfile # Points to the updated Dockerfile
    container_name: ckguru_backend
    volumes:
      - ./backend/app:/app/app # Hot-reload for backend code
      # Mounts for alembic are still useful for development to see changes
      # without rebuilding the image every time.
      - ./backend/alembic.ini:/app/alembic.ini
      - ./backend/alembic:/app/alembic
      - ./shared:/app/shared # Shared directory hot-reload 
    ports:
      - "8000:8000"
    env_file:
      - .env
    environment:
      STORAGE_BASE_PATH: /app/persistent_data
    depends_on:
      db:
        condition: service_healthy # IMPORTANT: Wait for DB healthcheck
      broker:
        condition: service_started # Adjust condition as needed
      redis:
        condition: service_started
      minio:
        condition: service_healthy

  #################
  # Ingestion Worker
  #################
  ingestion-worker:
    build:
      context: . # Build context is root
      dockerfile: worker/ingestion/Dockerfile # Path to specific Dockerfile
    container_name: ckguru_ingestion_worker
    volumes:
      - ./worker/ingestion/app:/app/app # Worker-specific code hot-reload
      - ./worker/ingestion/services:/app/services # Ingestion services hot reload
      - ./shared:/app/shared           # Shared code hot-reload
      - app_data:/app/persistent_data # Volume for git clones
    env_file:
      - .env
    environment:
      STORAGE_BASE_PATH: /app/persistent_data # Needed for clone path logic
    depends_on:
      - broker
      - db
      - minio # Does ingestion *need* minio? Maybe not directly. Keep if utils access it.
      - redis # For results backend if enabled

  #################
  # Dataset Worker
  #################
  dataset-worker:
    build:
      context: .
      dockerfile: worker/dataset/Dockerfile
    container_name: ckguru_dataset_worker
    volumes:
      - ./worker/dataset/app:/app/app
      - ./worker/dataset/services:/app/services
      - ./shared:/app/shared
      # No app_data needed unless it reads clones directly (it shouldn't)
    env_file:
      - .env
    depends_on:
      - broker
      - db
      - minio
      - redis

  #################
  # ML Worker
  #################
  ml-worker:
    build:
      context: .
      dockerfile: worker/ml/Dockerfile
    container_name: ckguru_ml_worker
    volumes:
      - ./worker/ml/app:/app/app         # ML-specific code hot-reload
      - ./worker/ml/services:/app/services # ML services hot-reload
      - ./shared:/app/shared             # Shared code hot-reload
      # Mount data volume if needed for temporary storage? Usually S3 is preferred.
      # - app_data:/app/persistent_data
    env_file:
      - .env
    environment:
      # Pass log level if needed, or rely on default in CMD
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      # Ensure CUDA devices are visible
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      # NVIDIA_VISIBLE_DEVICES: all # Often set by Docker runtime automatically with deploy key
    depends_on:
      - broker
      - db
      - minio
      - redis # If using Redis as result backend
    # <<< --- GPU Configuration --- >>>
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all # Or specify e.g., 1
              capabilities: [gpu]

  #################
  # Object Storage (MinIO)
  #################
  minio:
    image: minio/minio:RELEASE.2025-04-03T14-56-28Z # If cpu architecture does not support x86_64-v2 use RELEASE.2025-04-03T14-56-28Z-cpuv1 (cpuv1 variant)
    container_name: ckguru_minio
    env_file:
      - .env # Load MINIO_ROOT_USER, MINIO_ROOT_PASSWORD from .env
    environment:
      # These should match S3_ACCESS_KEY_ID and S3_SECRET_ACCESS_KEY in .env
      MINIO_ROOT_USER: ${S3_ACCESS_KEY_ID}
      MINIO_ROOT_PASSWORD: ${S3_SECRET_ACCESS_KEY}
    command: server /data --console-address ":9001" # Start server, data dir, console port
    ports:
      - "9000:9000" # API port
      - "9001:9001" # Console port
    volumes:
      - minio_data:/data # Persistent storage for MinIO buckets/objects
    healthcheck:
      test: ["CMD", "mc", "ready", "local"] # Use MinIO Client for healthcheck
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
  mc: # MinIO Client Helper Container (for creating bucket)
      image: minio/mc
      depends_on:
          minio:
              condition: service_healthy
      env_file:
          - .env # Need access keys
      entrypoint: >
          /bin/sh -c "
          echo 'Waiting for MinIO...' &&
          sleep 5 &&
          mc config host add local ${S3_ENDPOINT_URL:-http://minio:9000} ${S3_ACCESS_KEY_ID} ${S3_SECRET_ACCESS_KEY} --api S3v4 &&
          echo 'Checking if bucket ${S3_BUCKET_NAME} exists...' &&
          mc ls local/${S3_BUCKET_NAME} > /dev/null 2>&1 || (
              echo 'Bucket ${S3_BUCKET_NAME} does not exist. Creating...' &&
              mc mb local/${S3_BUCKET_NAME} &&
              echo 'Bucket ${S3_BUCKET_NAME} created.'
          ) || echo 'Bucket ${S3_BUCKET_NAME} already exists.'
          echo 'MinIO setup complete.'
          "

  #################
  # Frontend UI
  #################
  frontend:
    build:
      context: .
      dockerfile: frontend/Dockerfile
    container_name: ckguru_frontend
    ports:
      - "3000:3000" # Map host 3000 to container 3000 (Next.js default)
      - "80:80" # Map host 80 to container 80 (Nginx default)
    environment:
      # Ensure Next.js listens on all interfaces inside the container
      HOSTNAME: "0.0.0.0"
    depends_on:
      - backend # Usually depends on backend being available

  #################
  # Database
  #################
  db:
    image: postgres:15-alpine
    container_name: ckguru_db
    volumes:
      - postgres_data:/var/lib/postgresql/data # Persistent DB data
    env_file:
      - .env # For POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_DB
    ports:
      - "5432:5432" # Optional: Expose DB port to host for debugging
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 5s
      timeout: 10s
      retries: 5

  #################
  # Message Broker
  #################
  broker:
    image: rabbitmq:3.12-management-alpine
    container_name: ckguru_broker
    env_file:
      - .env # For RABBITMQ_DEFAULT_USER, RABBITMQ_DEFAULT_PASS
    ports:
      - "5672:5672"   # AMQP protocol port
      - "15672:15672" # Management UI port
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq # Persistent storage for RabbitMQ data

  #################
  # Result Backend (Redis)
  #################
  redis:
    image: redis:7-alpine
    container_name: ckguru_redis
    # ports: # Optional: Expose port 6379 to host for debugging if needed
    #   - "6379:6379"
    volumes:
      - redis_data:/data # Persistent storage for Redis data
    # Add a basic healthcheck (optional but good practice)
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  #################
  # Monitoring
  #################
  flower:
    image: mher/flower:1.2.0
    container_name: ckguru_flower
    #command: ["flower", "--broker=${CELERY_BROKER_URL}", "--persistent=True", "--db=/flower_data/flower.db"]
    volumes:
      - flower_data:/data # Persist Flower's state
    ports:
      - "5555:5555" # Flower web UI port
    env_file:
      - .env # Needs broker URL
    depends_on:
      - broker # Wait for the broker to be available
      - redis # Often useful to see result backend status too

#################
# Volumes
#################
volumes:
  postgres_data: # Stores PostgreSQL data
  app_data:      # Stores git clones for worker
  redis_data:    # Stores Redis data (optional, but good for persistence)
  minio_data:    # Stores MinIO data (buckets/objects) for datasets, models, etc.
  rabbitmq_data: # Stores RabbitMQ message data, user configs, etc.
  flower_data:   # Stores Flower monitoring DB