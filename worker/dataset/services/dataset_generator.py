# worker/dataset/services/dataset_generator.py
import logging
import traceback
from typing import List, Dict, Optional, Type

import s3fs
import pandas as pd
from sqlalchemy import select
from celery import Task
from celery.exceptions import Terminated

from shared.db.models import Dataset, Repository, BotPattern
from shared.db_session import get_sync_db_session
from shared.core.config import settings
from shared.schemas.enums import DatasetStatusEnum 
from shared.utils.task_utils import update_task_state


from .data_loader import DataLoader
# Import the factory function
from .cleaning_service_factory import get_cleaning_service
# Import the base service class for type hint
from .cleaning_rules.base import CleaningRuleBase, WORKER_RULE_REGISTRY, discover_rules
from .cleaning_service_base import BaseCleaningService
from .processing_steps import ProcessingSteps
from .output_writer import OutputWriter

logger = logging.getLogger(__name__)
logger.setLevel(settings.LOG_LEVEL.upper())

class DatasetGenerator:
    """
    Orchestrates the dataset generation process using the Template Method pattern.
    """

    def __init__(
        self,
        dataset_id: int,
        task_instance: Task,
        ):
        self.dataset_id = dataset_id
        self.task = task_instance
        self.session = None
        self.dataset: Optional[Dataset] = None
        self.repository: Optional[Repository] = None
        self.bot_patterns: List[BotPattern] = []
        self.dataset_config: Dict = {}
        self.feature_columns: List[str] = []
        self.target_column: Optional[str] = None
        self.object_storage_uri: Optional[str] = None
        self.output_writer: Optional[OutputWriter] = None
        self.cleaning_service: Optional[BaseCleaningService] = None # Store cleaning strategy instance
        self.processed_row_count: int = 0
        self.written_row_count: int = 0

    def _update_status(self, message: str, progress: int, state: str = 'STARTED'):
        """Helper to update Celery task progress and DB status message."""
        update_task_state(self.task, state, message, progress)
        # Update DB status message periodically within the session
        if self.session and self.session.is_active and self.dataset:
            try:
                # Re-attach dataset to session if needed, though it should be tracked
                # self.session.add(self.dataset)
                self.dataset.status_message = message[:1000] # Limit length
                self.session.commit()
                # Refresh dataset object after commit to get latest state? Optional.
                # self.session.refresh(self.dataset)
            except Exception as db_update_err:
                 # Log error but don't fail the whole task just for status update
                 logger.error(f"Failed to update DB status message: {db_update_err}")
                 self.session.rollback() # Rollback only the status update attempt

    # --- Template Steps (Primitives) ---

    def _load_configuration(self):
        """Loads dataset definition, repository info, bot patterns, and config."""
        logger.info(f"Task {self.task.request.id}: Step 1: Loading configuration...")
        self.dataset = self.session.get(Dataset, self.dataset_id)
        if not self.dataset:
            raise ValueError(f"Dataset definition {self.dataset_id} not found.")

        if self.dataset.status == DatasetStatusEnum.GENERATING and self.dataset.celery_task_id != self.task.request.id:
             # Check if another task ID is associated with the GENERATING status
             logger.warning(f"Dataset {self.dataset_id} is already being generated by task {self.dataset.celery_task_id}. Aborting this task.")
             raise ValueError(f"Dataset {self.dataset_id} generation already in progress by another task.")

        self.repository = self.session.get(Repository, self.dataset.repository_id)
        if not self.repository:
            raise ValueError(f"Repository {self.dataset.repository_id} not found for dataset {self.dataset_id}.")

        # Fetch bot patterns
        bot_stmt = select(BotPattern).where(
            (BotPattern.repository_id == self.repository.id) | (BotPattern.repository_id.is_(None))
        ).order_by(BotPattern.repository_id.nullslast(), BotPattern.id)
        self.bot_patterns = list(self.session.execute(bot_stmt).scalars().all())
        logger.info(f"Fetched {len(self.bot_patterns)} applicable bot patterns.")

        # Load and validate config
        self.dataset_config = self.dataset.config if isinstance(self.dataset.config, dict) else {}
        self.feature_columns = self.dataset_config.get('feature_columns', [])
        self.target_column = self.dataset_config.get('target_column', 'is_buggy')
        if not self.feature_columns:
            raise ValueError("Dataset configuration must specify 'feature_columns'.")
        
# --- Initialize Cleaning Service using Factory and CHECKING/RE-DISCOVERING registry ---
        # Check registry *within the worker process context*
        current_process_registry = WORKER_RULE_REGISTRY
        if not current_process_registry:
            # If it's empty here, discovery likely didn't persist across the fork.
            # Run discovery again within this process.
            logger.warning("Worker rule registry is empty. Attempting discovery within the task process...")
            # --- Make sure the path is correct relative to where this file is ---
            # Assuming implementations.py is in the same directory as base.py
            # Or provide the full path relative to the worker root if needed
            discover_rules(module_path="services.cleaning_rules.implementations") # Or adjust path if needed
            current_process_registry = WORKER_RULE_REGISTRY # Re-assign after discovery
            if not current_process_registry:
                # If it's STILL empty, there's a deeper problem (e.g., import error, no rules defined)
                logger.error("Rule registry is still empty after attempting discovery in worker process!")
                raise RuntimeError("Rule registry is empty even after discovery attempt, cannot initialize cleaning service.")
            else:
                logger.info(f"Successfully discovered {len(current_process_registry)} rules within the task process.")

        try:
            # Now use the (potentially re-populated) registry
            self.cleaning_service = get_cleaning_service(self.dataset_config, current_process_registry)
        except Exception as e:
             logger.error(f"Failed to initialize cleaning service: {e}", exc_info=True)
             raise ValueError("Failed to initialize cleaning service strategy.") from e

        # Set Generating status and task ID
        self.dataset.status = DatasetStatusEnum.GENERATING
        self.dataset.celery_task_id = self.task.request.id # Store current task ID
        self.session.commit()

        # Prepare output path and writer, clear existing output
        filename = f"dataset_{self.dataset_id}.parquet"
        self.object_storage_uri = f"s3://{settings.S3_BUCKET_NAME}/datasets/{filename}"
        logger.info(f"Output target: {self.object_storage_uri}")
        self.output_writer = OutputWriter(settings.s3_storage_options)
        self.output_writer.clear_existing(self.object_storage_uri)

        self._update_status("Configuration loaded. Querying data...", 5)

    def _process_batches(self) -> List[pd.DataFrame]:
        """Fetches data and applies batch-level processing steps."""
        logger.info(f"Task {self.task.request.id}: Step 2: Processing data in batches...")
        if not self.cleaning_service: # Should be initialized in _load_configuration
             raise RuntimeError("Cleaning service not initialized before processing batches.")

        batch_size = 1000
        data_loader = DataLoader(self.session, self.repository.id, self.bot_patterns)
        estimated_total = data_loader.estimate_total_rows()
        all_batches_data = []
        batch_num = 0

        for batch_df in data_loader.stream_batches(batch_size):
            batch_num += 1
            rows_in_batch = len(batch_df)
            self.processed_row_count += rows_in_batch
            progress = 5 + int(45 * (self.processed_row_count / estimated_total))
            self._update_status(f"Processing batch {batch_num} ({rows_in_batch} rows)...", progress)

            try:
                batch_df = ProcessingSteps.apply_file_filters(batch_df)
                if batch_df.empty: continue
                batch_df = ProcessingSteps.calculate_commit_stats(batch_df)
                parent_metrics_df = ProcessingSteps.get_parent_ck_metrics(self.session, batch_df)
                batch_df = batch_df.join(parent_metrics_df)
                batch_df = ProcessingSteps.calculate_delta_metrics(batch_df)
                # --- Use the cleaning service strategy ---
                batch_df = self.cleaning_service.apply_batch_rules(batch_df)
                # -----------------------------------------
                batch_df = ProcessingSteps.drop_missing_parents(batch_df)

                if not batch_df.empty:
                     all_batches_data.append(batch_df)
            except Exception as batch_err:
                 logger.error(f"Error processing batch {batch_num}: {batch_err}", exc_info=True)
                 raise RuntimeError(f"Failed processing batch {batch_num}") from batch_err

        self._update_status("Batch processing complete.", 50)
        return all_batches_data

    def _process_globally(self, all_batches_data: List[pd.DataFrame]) -> pd.DataFrame:
        """Combines batches and applies global processing steps."""
        logger.info(f"Task {self.task.request.id}: Step 3: Applying global processing...")
        if not self.cleaning_service: # Should be initialized
            raise RuntimeError("Cleaning service not initialized before global processing.")

        if not all_batches_data:
             logger.warning("No data accumulated from batches.")
             self._update_status("No data after batch processing.", 90)
             return pd.DataFrame()

        self._update_status("Combining batches...", 51)
        full_df = pd.concat(all_batches_data, ignore_index=True)
        del all_batches_data
        logger.info(f"Combined DataFrame shape for global processing: {full_df.shape}")

        # --- Use the cleaning service strategy ---
        full_df = self.cleaning_service.apply_global_rules(full_df)
        # -----------------------------------------

        self._update_status("Global rules applied.", 90)
        return full_df

    def _select_and_write_output(self, final_df: pd.DataFrame):
        """Selects final columns and writes the output Parquet file."""
        logger.info(f"Task {self.task.request.id}: Step 4: Finalizing and writing output...")
        self._update_status("Selecting final features...", 91)

        try:
            final_df_selected = ProcessingSteps.select_final_columns(final_df, self.feature_columns, self.target_column)
        except ValueError as e:
            # Handle case where no columns remain
            logger.error(f"Failed selecting final columns: {e}")
            raise # Propagate error to fail the task
        except Exception as e:
            logger.error(f"Unexpected error selecting final columns: {e}", exc_info=True)
            raise # Propagate error

        self.written_row_count = len(final_df_selected)

        self._update_status("Writing final dataset...", 95)
        self.output_writer.write_parquet(final_df_selected, self.object_storage_uri)

        # --- Final Success Update in DB ---
        # This commit should happen only if write_parquet succeeds
        self.dataset.status = DatasetStatusEnum.READY
        self.dataset.storage_path = self.object_storage_uri
        self.dataset.status_message = f"Dataset generated successfully. {self.written_row_count} rows written."
        self.session.add(self.dataset) # Ensure it's added if detached
        self.session.commit()
        logger.info(f"Dataset {self.dataset_id} status updated to READY.")

        # Update Celery task status *after* DB commit
        self._update_status(self.dataset.status_message, 100, state='SUCCESS')
        logger.info(f"Task {self.task.request.id}: Dataset generation successful.")

    # --- Error Handling & Cleanup ---
    def _cleanup_and_fail_db(self, error_message: str, detailed_error: Optional[str] = None):
        """Updates dataset status to FAILED and attempts storage cleanup."""
        logger.warning(f"Running cleanup for dataset {self.dataset_id} due to error/termination.")
        final_status_message = f"FAILED: {error_message}"
        if detailed_error:
             final_status_message += f"\nDetails: {detailed_error[:800]}" # Limit length

        # Use a NEW session for cleanup in case the main one is compromised
        try:
            with get_sync_db_session() as cleanup_session:
                cleanup_dataset = cleanup_session.get(Dataset, self.dataset_id)
                if cleanup_dataset:
                    cleanup_dataset.status = DatasetStatusEnum.FAILED
                    cleanup_dataset.status_message = final_status_message[:1000]
                    cleanup_dataset.celery_task_id = self.task.request.id # Ensure task ID is set even on failure
                    cleanup_session.commit()
                    logger.info(f"Updated dataset {self.dataset_id} status to FAILED in DB.")
                else:
                     logger.error(f"Could not find dataset {self.dataset_id} in DB during error cleanup.")
        except Exception as db_update_err:
            logger.error(f"CRITICAL - Failed to update dataset {self.dataset_id} status to FAILED in DB: {db_update_err}")

        # Attempt to cleanup storage (best effort)
        if self.output_writer and self.object_storage_uri:
            try:
                # Use a simple check/delete, writer might not be fully initialized if error was early
                fs = s3fs.S3FileSystem(**settings.s3_storage_options)
                s3_path = self.object_storage_uri.replace("s3://", "")
                if fs.exists(s3_path):
                    logger.warning(f"Deleting potentially incomplete/failed output file: {self.object_storage_uri}")
                    fs.rm(s3_path)
            except Exception as cleanup_err:
                logger.error(f"Failed to cleanup output file {self.object_storage_uri} after error: {cleanup_err}")

    # --- Template Method ---
    def generate(self) -> Dict:
        """Executes the dataset generation pipeline."""
        task_id = self.task.request.id
        logger.info(f"Task {task_id}: Starting DatasetGenerator for ID {self.dataset_id}")
        final_result = { # Default result structure
            'dataset_id': self.dataset_id,
            'status': 'FAILED',
            'rows_written': 0,
            'path': None,
            'error': 'Generation did not complete.'
        }

        try:
            # Use context manager for the main session
            with get_sync_db_session() as session:
                self.session = session
                self._load_configuration()           # Step 1
                processed_batches = self._process_batches() # Step 2
                global_df = self._process_globally(processed_batches) # Step 3
                self._select_and_write_output(global_df) # Step 4 (includes final DB commit on success)

            # Update final result on success
            final_result['status'] = 'SUCCESS'
            final_result['rows_written'] = self.written_row_count
            final_result['path'] = self.object_storage_uri
            final_result.pop('error', None) # Remove error key on success
            
            return final_result

        except Terminated as term_exc:
            error_msg = "Task terminated by revoke request."
            logger.warning(f"Task {task_id}: {error_msg} (Details: {term_exc})")
            self._cleanup_and_fail_db(error_msg) # Updates DB status to FAILED
            final_result['error'] = error_msg
            # Don't update Celery status here, Terminated exception handles it
            raise # Re-raise Terminated

        except Exception as e:
            error_msg = f"Dataset generation failed: {type(e).__name__}"
            detailed_error = f"{error_msg}: {e}\n{traceback.format_exc()}"
            logger.error(f"Task {task_id}: {detailed_error}", exc_info=False) # Already included in detailed_error
            # Update Celery task state to FAILURE immediately
            #update_task_state(self.task, 'FAILURE', error_msg, 0)
            # Cleanup DB and storage
            self._cleanup_and_fail_db(error_msg, detailed_error)
            final_result['error'] = detailed_error # Store detailed error in result
            # Do not re-raise here; allow Celery to mark as failed based on update_state
            raise e